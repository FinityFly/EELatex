% Chapter Template

\chapter{Introduction} % Main chapter title

\label{Introduction} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\section{Abstract}

Automatic speech recognition (ASR) is the ability of a machine to recognize language from human speech and convert it into written text. Such a task is difficult due to the sheer complexity of any spoken language, as well as the varying and unpredictable nature of different people and their speaking habits. Even big-name companies such as Alphabet, Microsoft, and Baidu, who pride themselves in their sophisticated multi-million dollar speech recognition systems, have been countlessly ridiculed for their misinterpretations. However, due to the endless number of possible and practical applications of ASR technology in our developing world, major corporations still aspire to develop an accurate yet optimized speech recognition system for their products and services. For instance, Baidu spent a total of \$4.7 billion dollars on research and development, of which a large portion headed into developing their voice assistant, DuerOS. Hence, the implementation of ASR can improve the ergonomics of any task that requires human interaction, such as personal assistants, smart healthcare and household appliances, and media transcription to support people with disabilities or language barriers. 
\newline\par
Significant development for ASR began in the 50s, with the innovation of Bell’s Audrey and IBM’s Shoebox. These systems fell into a category of ASR systems that recognized individual words to an isolated vocabulary using manually set parameters for phoneme recognition. By the end of the decade, ASR technology could distinguish words with a maximum of four vowels and nine consonants and by the 80s, systems could recognize up to 3000 words, which is around the vocabulary of a six-year-old child. It wasn’t until the late 90s that researchers transitioned outwards of phoneme detection to Hidden Markov Models (HMM) and focused on natural language processing. Deep learning concepts and technologies emerged and were investigated as a solid alternative to HMMs, to the extent to which it is prominently used today.
\newline\par
However, a plausible deep learning system that can recognize continuous speech within a low margin of error is very strenuous to build. A well-designed system can consist of multiple complex layers that can be very complicated to integrate into one another. Nowadays, large tech companies boast an accuracy of almost 95%, which is still fairly low in the context of deep learning. One of these layers is commonly a feature extraction layer that optimizes an input waveform and isolates the distinct resonant qualities of the human vocal tract. Modern speech recognition models widely use Mel-Frequency Cepstrum Coefficients (MFCC) as a pre-processing layer. Yet, MFCCs often have shortcomings when processing audio signals with excess background noise due to their sensitive nature.

%-----------------------------------
%	RATIONALE
%-----------------------------------
\section{Rationale}

This report aims to investigate the extent to which Mel-Frequency Cepstrum Coefficients outperform other feature extraction methods in automatic speech recognition systems. I will assess the performances of MFCCs, Discrete Wavelet Transforms (DWT), Linear Predictive Coding (LPC), and Perceptual Linear Prediction (PLP), as well as the absence of a feature extraction layer, processed through identical recurrent neural network architectures. From this investigation, I will better understand the effectiveness and limitations of different speech feature extraction methods and inform my forthcoming decisions in ASR neural networks.
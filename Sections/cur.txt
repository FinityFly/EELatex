% Chapter Template

\chapter{Analysis \& Conclusions} % Main chapter title

\label{AnalysisConclusions} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	EXPERIMENTAL ANALYSIS AND TRENDS
%----------------------------------------------------------------------------------------

\section{Experimental Analysis and Trends}

\begin{figure}[th]
    \centering
    \includesvg[width=\textwidth]{Figures/ComparisonOfAccuracy.svg}
    \caption[ComparisonOfAccuracy]{A comparison of extraction algorithms’ accuracies against each other}
    \label{fig:ComparisonOfAccuracy}
\end{figure}

In the visualizations of the experiment results above, a lot can be said about to what extent MFCCs outperform DWTs, LPCs, and PLPs. As observed in (Figure~\ref{fig:ComparisonOfAccuracy}), Mel spectrograms vastly outperformed all of the feature extraction algorithms in terms of accuracy when there is no background noise present, but when taking time into account, MFCCs are slightly faster to process. Nonetheless, usually during practical implementations of speech recognition, unlike these trials, there will most likely be less than eight minutes of total audio data to transcribe (500 trials of 1-second audio clips), making time complexity, in this case, less important than in other programming scenarios. It also should be mentioned that the pre-processing time during model training is irrelevant to the analysis of these feature extraction algorithms, as solely the final product of these trials is assessed.
\par
There is also a clear correlation between the performances of the spectrograms and the prediction models in various aspects. The spectrograms, including MFCCs, were much more negatively affected by the addition of background noise, which confirms the speculations of such found during my search, but still performed better than LPCs and PLPs. This could be due to the fact that prediction models such as PLP rely much more on extracting relational data instead of literal data.
\par
The experimental process could also be improved, as it led to a few sources of error: the first of which lies in the consistency of the speed of the computer the tests were ran on. This error can be disregarded if an uncertainty of ±0.1 is added to the running times of each trial. Another possible source of error is overfitting of the training data set from especially fast learning, which may have caused a lower than optimal performance on test datasets than could have been corrected if the model stopped training earlier. I did not account for this error since I can derive my conclusions form purely comparative data between the feature extraction algorithms.
\par
I believe that it would be worthwhile to examine the extent to which MFCCs can perform well in noise-heavy environments with the use of noise cancellation software; raw experimental results do not draw out clear conclusions on whether MFCCs should be used if a system expects a high magnitude of background noise.

%-----------------------------------
%	FURTHER RESEARCH OPPORTUNITIES AND ASPIRATIONS
%-----------------------------------

\section{Further Research Opportunities and Aspirations}

There are several further research opportunities and aspirations I’d like to explore in the future regarding and related to my research question. The speech feature extraction algorithms I examined in this paper are a portion of the most conventionally used computer scientists use today for speech recognition. Even so, there are some modern and advanced yet ambiguous speech feature extraction algorithms out there that may perform even better than the ones investigated in this report but don’t have the recognition to gain traction in the field of ASR. Investigating the extent to which these are viable as a replacement for MFCCs could yield interesting results. 
\par
Furthermore, in the future, I wish to extensively develop this RNN model further until it’s useable in practical cases. I could research the coupling of bi-directional LSTMs with these feature extraction methods (which are very popular in NLP), work with deep LSTM networks, or even experiment with Gated Recurrent Units or transforms to see how each compares.

%-----------------------------------
%	CONCLUSIONS
%-----------------------------------

\section{Conclusions}

This paper presented several conclusions regarding the research question at hand through an analysis of the performances of different speech feature extraction algorithms. The results show that although MFCCs are more optimized and contain denser key vocal features of a waveform, Mel spectrograms overall possess the highest degree of accuracy while maintaining a reasonable processing time with large inputs.































In the visualizations of the experiment results above, much can be inferred about how differing feature extraction algorithms perform against each other. As observed in (Figure~\ref{fig:ComparisonOfAccuracy}), spectrograms, Mel spectrograms, and MFCCs performed similarly, with similar results in loss and WER, while DWT’s performance is noticeably worse than any other algorithm in terms of both accuracy and time spent. While other speech extraction algorithms had an ending WER of roughly 0.63, DWT’s average WER in epoch 5 is 0.74. DWT’s poor performance could be attributed to two possible causes: either the algorithm is inefficient in extracting audio features or there is error present in implementing the algorithm. DWT outputs coefficients whose values indicate the overall resemblance of a wavelet to any window of a specific waveform. The number of coefficients can vary but are generally very low compared to the number of data present in spectrograms, resulting in an inevitably lossy output within this investigation. Additionally, it is the sole algorithm that did not use Tensorflow’s signal library and instead PyWavelets, which may have caused the significant disparity in execution time.
\par
Concerning the performances of spectrograms, Mel spectrograms, and MFCCs, it is seen in (Figure~\ref{fig:ComparisonOfAccuracy}) and (Figure~\ref{fig:ComparisonOfTime}) that their accuracies and times are very similar, but have small discrepancies that are important to distinguish. Mel spectrograms and MFCCs generally achieve better validation loss than raw spectrograms while spectrograms and Mel spectrograms have slightly lower word error rates than MFCCs.
\par
The experimental process could also be improved, as it led to a few sources of error: the first of which lies in the consistency of the speed of the computer the tests were run on. In Table~\ref{spectro_time} and~\ref{dwt_time} found in \autoref{AppendixB}, the large standard deviation of the spectrogram and DWT trials shows that there a large uncertainty in measuring execution time can exist, possibly due to the varied use of the training CPU that occurred during the trials. This error can be disregarded if an uncertainty of ±1 second is added to the running times of each trial.
\par
I believe that it would be worthwhile to examine the extent to which MFCCs can perform well in noise-heavy environments with the use of noise cancellation software; raw experimental results do not draw out clear conclusions on whether MFCCs should be used if a system expects a high magnitude of background noise.
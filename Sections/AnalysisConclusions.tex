% Chapter Template

\chapter{Analysis \& Conclusions} % Main chapter title

\label{AnalysisConclusions} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	EXPERIMENTAL ANALYSIS AND TRENDS
%----------------------------------------------------------------------------------------

\section{Experimental Analysis and Trends}

\begin{figure}[th]
    \centering
    \includesvg[width=\textwidth]{Figures/ComparisonOfAccuracy.svg}
    \caption[ComparisonOfAccuracy]{A comparison of extraction algorithms’ accuracies against each other}
    \label{fig:ComparisonOfAccuracy}
\end{figure}

In the visualizations of the experiment results above, much can be inferred about how differing feature extraction algorithms perform against each other. As observed in (Figure~\ref{fig:ComparisonOfAccuracy}), spectrograms, Mel spectrograms, and MFCCs performed similarly, with similar results in loss and WER, while DWT’s performance is noticeably worse than any other algorithm in terms of both accuracy and time spent. While other speech extraction algorithms had an ending WER of roughly 63\%, DWT’s average WER in epoch 5 is 74\%. DWT’s poor performance could be attributed to two possible causes: either the algorithm is inefficient in extracting audio features or there is error present in implementing the algorithm. DWT outputs coefficients whose values indicate the overall resemblance of a wavelet to any window of a specific waveform. The number of coefficients can vary but are generally very low compared to the number of data present in spectrograms, resulting in an inevitably lossy output within this investigation. Additionally, it is the sole algorithm that did not use Tensorflow’s signal library and instead PyWavelets, which may have caused the significant disparity in execution time; this factor is further elaborated below.
\par
Concerning the performances of spectrograms, Mel spectrograms, and MFCCs, it is seen in (Figure~\ref{fig:ComparisonOfAccuracy}) and (Figure~\ref{fig:ExecutionTimes}) that their accuracies and times are very similar, but have small discrepancies that are important to distinguish. Mel spectrograms and MFCCs generally achieve better validation loss than raw spectrograms by a value of ~4 while spectrograms and Mel spectrograms have slightly lower word error rates than MFCCs, by nearly 2\%. Since the performance of Mel spectrograms remained consistently efficient in both metrics of accuracy, Mel spectrograms overall performed the best among all feature extraction algorithms explored in this study, though to a little extent.
\par
Furthermore, comparing the time complexity of each algorithm reveals that MFCCs are the most efficient. This could be a result of the less trainable parameters needed in the neural network to process the compressed output of MFCCs. To elaborate, a time step of data produced with MFCC that is passed to an LSTM cell would consume around 21n spaces of data, with n being the number of time steps. In contrast, a spectrogram would normally take up nearly 193n spaces of data per time step\footnote{For a frame step of 160, frame length of 256, and fast Fourier transform length of 384}, depending on the frequency range desired while computing the STFT, which would require levels of magnitude higher levels of computing required to process an audio signal.

%-----------------------------------
%	FURTHER RESEARCH OPPORTUNITIES AND ASPIRATIONS
%-----------------------------------

\section{Further Research Opportunities and Aspirations}

The experimental process could be improved, as it led to a few sources of error: the first of which lies in the consistency of the speed of the computer the tests were run on. In Table~\ref{spectro_time} and~\ref{dwt_time} found in \autoref{AppendixB}, the large standard deviation of the spectrogram and DWT trials shows that there is a large uncertainty in measuring execution time can exist, possibly due to the varied use of the training CPU that occurred during the trials. This error can be disregarded if an uncertainty of ±1 second is added to the running times of each trial as a result of systematic irregularity in hardware.
\par
Another source of error mentioned above is the inconsistency in the selection of audio processing libraries used to carry out the feature extraction algorithms. Consequently from the limited number of options available on any single audio analysis library that doesn’t include the entirety of my independent variables, such as Librosa, multiple libraries had to be used, which altogether reduces the uniformity within my methodology. This error can be eliminated by implementing the algorithms manually without using third-party libraries.
\par
To expand the scope of future investigations regarding the research topic, there are several improvements that could be implemented. To attain more accurate, representative data and better overall performance of each algorithm in the neural network, more epochs of training data could have been trained if hardware limitations were not present. Additionally, a downside of the current dataset is that all of the NLC speech is spoken by one speaker, which is unrealistic in practical scenarios. Obtaining a dataset with a variety of speakers would better reflect the performances of these algorithms through more realistic analyses.\\
\par
There are several research opportunities that could be explored regarding the research question. The speech feature extraction algorithms examined in this paper are a portion of the most conventionally used computer scientists use today for speech recognition. Even so, there are some modern and advanced yet ambiguous speech feature extraction algorithms out there that may perform even better than the ones investigated in this report but don’t have the recognition to gain traction in the field of ASR. Investigating the extent to which these are viable as a replacement for MFCCs could yield interesting results. 
\par
In the future, it would be worthwhile to examine the extent to which MFCCs can perform well in noise-heavy environments with the addition of white noise and the use of noise reduction/cancellation systems; the experimental results currently presented do not draw out clear conclusions on whether MFCCs should be used if a system expects a high magnitude of background noise.

%-----------------------------------
%	CONCLUSIONS
%-----------------------------------

\section{Conclusions}

This paper presented several conclusions regarding the research question at hand through an analysis of the performances of different speech feature extraction algorithms. The results show that although MFCCs are considerably optimized and contain denser key vocal information of a waveform, Mel spectrograms overall possess the highest degree of accuracy while maintaining a reasonable processing time with large inputs. In a more practical sense, the use of spectrograms, Mel spectrograms, and MFCCs are generally all viable within a speech feature extraction layer in most non-industry ASR neural networks.
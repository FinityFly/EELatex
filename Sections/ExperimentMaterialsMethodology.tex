% Chapter Template

\chapter{Experiment Materials \& Methodology} % Main chapter title

\label{ExperimentMaterialsMethodology} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

In this paper, heavy importance is placed on primary experimental observations and data, as well as a complete understanding of every operation that is carried out. For those reasons, deep learning APIs like Keras or Torch were not implemented, though this might have an effect on the overall performance of my neural network due to the great amount of optimization such open-source projects have. Nevertheless, this doesn’t have an impact on the results of my investigation, as the network will remain a control variable throughout all trials. The dependent variables are the speech feature extraction algorithms (MFCC, DWT, LPC, and PLP) and the background noise of each sample (none, white noise, and miscellaneous/random). To properly appraise the extent to which MFCCs can extract speech features, background noise must be a variable due to being a known downside to them. The independent variables are the accuracy and time allotted to each network when processing the test dataset. 

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\section{Datasets Used}

I used “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition” published by Pete Warden to train my neural network. The dataset contains 6500 sets of one-word dialogue paired with its transcription as the true output. I later altered this dataset by concatenating copies of the audio data with various types of background noise using Librosa.
\newline\par
However, as I plan to use continuous speech recognition neural network architectures, using this dataset is only a simplistic and intuitive demonstration of what the model is capable of doing, in addition to an effort by myself to not be too ambitious and over-extend my experimentation for this report. Further developments of this model would feature and employ other open-source datasets that include multi-word speech data, such as podcasts or court hearings. I hope to alter this later on.


%-----------------------------------
%	PRE-PROCESSING
%-----------------------------------
\section{Pre-Processing}

The dataset provided the sound files in .wav format, which made it very simple to extract its waveform data. I used ARSS’s audio analysis program to perform an FFT on the dataset and obtain the respective DFT spectrograms. Going on, I used Librosa to retrieve the spectrograms' speech feature data to ultimately feed into the network.

%-----------------------------------
%   NETWORK ARCHITECTURE
%-----------------------------------
\section{Network Architecture}

This report uses an end-to-end recurrent phoneme-based neural network consisting of a layer of LSTM cells and a final decoding node to process and display the output. The LSTM model consists of 256 hidden units that accept an input that’s of shape (3, 26) and output a one-hot matrix of shape (46, 1) at each time step. The output is a probability matrix of all the 44 English phonemes, following ARPABET conventions, that the model predicts the audio signal refers to at that point in time, as well as a space and a “blank” character that are incorporated to separate concurrent letters in the combined output. I choose to use a phoneme classification system over character-based or word-based since I believe there is more flexibility and accuracy established when you dissect a sentence into the fundamental building blocks of spoken language and build from the ground up once the phonemes are selected. A working result of this model would output a sequence of slurred letters that, once decoded with Connectionist Temporal Classification (CTC) loss, would output a word; the model could output “ww_eee_ee_p”, which suggests that the original audio input is of a person saying “weep”. After the fact, these phonemes are then fed into an English language model to convert the outputted phonemes into written text.
\newline\par
The model was trained on 5000 pieces of training data, 10 epochs of such, with a learning rate of 0.01 and evaluated by three validation datasets, for each of the three trials, that held 500 pieces of data to determine the accuracy of each model.

%-----------------------------------
%	EXPERIMENTAL PROCESS
%-----------------------------------
\section{Experimental Process}

Before I elaborate on my experimental process, it is essential to clarify some notable semantics of this investigation. The general performance basis of a model is assessed on a synthesis of its accuracy and its efficiency, though with a greater emphasis on accuracy. Whether an ASR system is successful is determined by whether its accuracy is greater than 50\% when recognizing NLC and whether its processing latency is less than five seconds. The accuracy of a model is calculated appertaining to its loss from stochastic CTC and the efficiency of a model is gauged on a comparison of its estimated time complexity and a measured difference of time. 
\newline\par
I will hold three trials for each set of independent variables to answer my research question. The performance of raw spectrograms, MFCCs, DWTs, LPCs, and PLPs will be assessed against each other in three different dataset variants: with no background noise, with white noise, and with other miscellaneous background noises, amounting to a total of 45 trials.